{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"/content/drive/MyDrive/Colab Notebooks/dataset.zip\"\n",
    "extract_path = \"/content/drive/MyDrive/Colab Notebooks/Ewaste1ws\"\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Extracted to {extract_path}\")\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"/content/drive/MyDrive/Colab Notebooks/Ewaste roboflow.zip\"\n",
    "extract_path = \"/content/drive/MyDrive/Colab Notebooks/Ewaste2\"\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Extracted to {extract_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def create_splits(dataset_path, output_path, train_ratio=0.7, val_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Split a dataset into train, test, and validate folders.\n",
    "\n",
    "    Args:\n",
    "        dataset_path: Path to the original dataset (Ewaste1).\n",
    "        output_path: Path to save the split dataset.\n",
    "        train_ratio: Proportion of data for training.\n",
    "        val_ratio: Proportion of data for validation.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    splits = ['train', 'test', 'validate']\n",
    "    for split in splits:\n",
    "        os.makedirs(os.path.join(output_path, split, 'images'), exist_ok=True)\n",
    "\n",
    "    # Iterate through each class folder\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_dir = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            random.shuffle(files)\n",
    "\n",
    "            train_split = int(len(files) * train_ratio)\n",
    "            val_split = int(len(files) * (train_ratio + val_ratio))\n",
    "\n",
    "            for i, file_name in enumerate(files):\n",
    "                src_path = os.path.join(class_dir, file_name)\n",
    "                if i < train_split:\n",
    "                    dest_path = os.path.join(output_path, 'train', 'images', file_name)\n",
    "                elif i < val_split:\n",
    "                    dest_path = os.path.join(output_path, 'validate', 'images', file_name)\n",
    "                else:\n",
    "                    dest_path = os.path.join(output_path, 'test', 'images', file_name)\n",
    "\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Paths\n",
    "ewaste1_path = '/content/drive/MyDrive/Colab Notebooks/Ewaste1ws'\n",
    "output_path = '/content/drive/MyDrive/Colab Notebooks/Ewaste1'\n",
    "\n",
    "# Create splits\n",
    "create_splits(ewaste1_path, output_path)\n",
    "print(\"Splits created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def combine_datasets(ewaste1_path, roboflow_path, output_path):\n",
    "    \"\"\"Combines Ewaste1 (image classification structure) with Roboflow (YOLO) dataset.\"\"\"\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, \"images\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path, \"labels\"), exist_ok=True)\n",
    "\n",
    "    img_count = 0\n",
    "\n",
    "\n",
    "    print(\"Processing Ewaste1...\")\n",
    "    for class_name in os.listdir(ewaste1_path):\n",
    "        class_dir = os.path.join(ewaste1_path, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            for img_file in os.listdir(class_dir):\n",
    "                if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    img_path = os.path.join(class_dir, img_file)\n",
    "                    new_img_name = f\"ewaste1_{img_count}.jpg\"\n",
    "                    new_img_path = os.path.join(output_path, \"images\", new_img_name)\n",
    "                    shutil.copy(img_path, new_img_path)\n",
    "\n",
    "\n",
    "                    label_content = f\"{class_number_map[class_name]} 0.5 0.5 1 1\\n\"\n",
    "                    label_filename = f\"ewaste1_{img_count}.txt\"\n",
    "                    label_path = os.path.join(output_path, \"labels\", label_filename)\n",
    "                    with open(label_path, \"w\") as f:\n",
    "                        f.write(label_content)\n",
    "\n",
    "                    img_count += 1\n",
    "\n",
    "\n",
    "    print(\"Processing Roboflow dataset...\")\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        split_path = os.path.join(roboflow_path, split)\n",
    "        if os.path.isdir(split_path):\n",
    "            img_dir = os.path.join(split_path, \"images\")\n",
    "            label_dir = os.path.join(split_path, \"labels\")\n",
    "\n",
    "            if os.path.exists(img_dir) and os.path.exists(label_dir):\n",
    "\n",
    "                for filename in os.listdir(img_dir):\n",
    "                    img_path = os.path.join(img_dir, filename)\n",
    "                    new_img_name = f\"roboflow_{img_count}_{filename}\"\n",
    "                    new_img_path = os.path.join(output_path, \"images\", new_img_name)\n",
    "                    shutil.copy(img_path, new_img_path)\n",
    "\n",
    "\n",
    "                    label_file = filename.replace(os.path.splitext(filename)[1], \".txt\")\n",
    "                    label_path = os.path.join(label_dir, label_file)\n",
    "                    if os.path.exists(label_path):\n",
    "                        new_label_path = os.path.join(output_path, \"labels\", f\"roboflow_{img_count}_{label_file}\")\n",
    "                        shutil.copy(label_path, new_label_path)\n",
    "                    img_count += 1\n",
    "\n",
    "    print(\"Combination Complete!\")\n",
    "\n",
    "def ensure_directory_exists(path):\n",
    "    \"\"\"Creates a directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def create_class_number_map(path):\n",
    "    \"\"\"Create a map of class names to class indices (integers)\"\"\"\n",
    "\n",
    "    class_names = sorted(os.listdir(path))\n",
    "    class_number_map = {class_name: i for i, class_name in enumerate(class_names)}\n",
    "    return class_number_map\n",
    "\n",
    "\n",
    "ewaste1_path = '/content/Ewaste1'\n",
    "roboflow_path = '/content/Ewaste2'\n",
    "output_path = '/content/combined_dataset'\n",
    "\n",
    "\n",
    "os.makedirs(output_path, exist_ok = True)\n",
    "\n",
    "\n",
    "class_number_map = create_class_number_map(ewaste1_path)\n",
    "print(class_number_map)\n",
    "\n",
    "ensure_directory_exists(ewaste1_path)\n",
    "\n",
    "combine_datasets(ewaste1_path, roboflow_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision ultralytics matplotlib\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "repo_path = \"/content/drive/MyDrive/ColabNotebooks/yolov5\"\n",
    "if not os.path.exists(repo_path):\n",
    "    !git clone https://github.com/ultralytics/yolov5 {repo_path}\n",
    "\n",
    "%cd {repo_path}\n",
    "\n",
    "!git pull\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"CUDA is available. Training on GPU.\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA is not available. Training on CPU.\")\n",
    "\n",
    "dataset_path = \"/content/drive/MyDrive/ColabNotebooks/combined_dataset/dataset.yaml\"\n",
    "if not os.path.exists(dataset_path):\n",
    "    raise FileNotFoundError(f\"Dataset YAML file not found at {dataset_path}\")\n",
    "\n",
    "!python train.py \\\n",
    "    --img 640 \\\n",
    "    --batch 16 \\\n",
    "    --epochs 10 \\\n",
    "    --data {dataset_path} \\\n",
    "    --weights yolov5s.pt \\\n",
    "    --device {device}\n",
    "\n",
    "best_pt_path = f\"{repo_path}/runs/train/exp/weights/best.pt\"\n",
    "\n",
    "if os.path.exists(best_pt_path):\n",
    "    print(f\"'best.pt' found at {best_pt_path}\")\n",
    "\n",
    "    drive_save_path = \"/content/drive/MyDrive/trained_model.pt\"\n",
    "    !cp {best_pt_path} {drive_save_path}\n",
    "    print(f\"Model weights saved to {drive_save_path}\")\n",
    "else:\n",
    "    print(\"Model weights could not be saved because 'best.pt' does not exist. Check training logs for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path='/content/drive/MyDrive/ColabNotebooks/yolov51/yolov5s.pt', force_reload=True)\n",
    "\n",
    "img_path = '/content/drive/MyDrive/ColabNotebooks/download.jpeg'\n",
    "\n",
    "results = model(img_path)\n",
    "\n",
    "results.print()\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
